\subsection{Job management}
\label{sec:jobmanager}

Job management has three parts, which are decoupled and implemented in scripts
in keeping with the SWIM overall software approach. The simplest is a user
interface for monitoring, and that is part of the SWIM portal's event
monitoring. The second part is the portal part that takes a specified component
(which at this stage are separate executables) and launches it on a specified
machine using the user's credentials for authentication. This part has been
reused from the National Fusion Grid Collaboratory, and is probably the most
onerous one for end users to get set up - but for current NFGC users SWIM will
re-use their existing grid credentials.  

The third part of job managment is the negotiation with (possibly remote) batch
management systems.
% The following was written by Sam as an outline of the batch management script
% it is basically plain text and I will make it pretty using latex later.
The batch management script {\bf batch\_mgmt\_script.py} listed in
Appendix \ref{batchscript} does this using
a Python class {\em fsp\_job}.  The class creates
{\em fsp\_job} objects which provide a common interface to PBS, SLURM, or
Unix processes. This shields users from the details of exactly how to run a
program on a specific machine.  More importantly it provides a mechanism
later on for us to submit a series of executions without having to wait
through the batch queue for each one separately. Note that this part
is {\em not} yet implemented, but the batch manager negotiator can be upgraded
to it without requiring changes to the rest of the SWIM software.

The batch negotiator includes active monitoring, checking with the batch manager
(or host OS) for the status of a job and publishing it via the event system
back to the SWIM portal - or to another event channel subscriber.

Information about the system the job is running on, what kind of job it is, the
resource requirements of the job and exact script to run are necessary for job
launch.  This information is stored in the {\bf SWIM\_config} file as a text
file of name-value pairs.  It is then parsed by the batch negotiator and run
according to the given parameters.
% We are going to have a bunch of config files. At least one for the overall
% SWIM configuration (i.e., above the individual components) and one for each
% individual component. Even I got confused about which one to use for which.
% Need to clarify that this is a per-component configuration script.

The batch negotiator allows running different codes on various systems just by
changing a few parameters in the {\bf SWIM\_config} file.  It also allows
configuring the codes to run with different versions of MPI or with different
batch managers if they are available on one machine but not another.
% The script is flexible and modular
% in that it can be extended and changed easily, then the new version is used
% everywhere without having to touch all of the different codes unnecessarily.

A detailed description of the batch negotiator, its data fields, and design
follows but most users can safely skip over them.
\begin{itemizer}
\item Data member, {\em my\_vals}, is a dictionary type that stores the
  following data:
  \begin{itemizer}
  \item  {\em batch\_mgr} : can be SLURM, or PBS, if something else is
    specified the job will be run as a regular program.  Programs are run
    differently by different batch managers.  For example, the command to
    submit a job to the queue is {\em srun} in SLURM, and {\em qsub} in PBS.
  \item  {\em num\_nodes} : this is only used when {\em batch\_mgr} is set to
    SLURM or PBS.  It is needed to specify the number of nodes needed to run
    the program at submission time.
  \item  {\em executable} : this is the name of the script file to run.  The
    script file must contain the full executable statement.  For example: ``ls''.  It is 
    recommended to use full paths.
  \item {\em jobid} : this will be set when the job is submitted.  It is used
    to monitor and cancel the job.
  \item  {\em status} : the status is set at runtime and is updated by the
    monitor function.  There are five states that are used:
    \begin{itemizer}
    \item  done : this indicates that the job no longer needs to run.  In the
      {\em \_\_init\_\_} function the state is set to done.  This corresponds to SLURM's CD
      and TO states, PBS's E state, and the UNIX command {\em ps}'s X and Z states.
    \item  trans : this indicates that the job is in some kind of transition.
      This corresponds to SLURM's CG state, PBS's T state, and the UNIX command
      {\em ps}'s T state.
    \item  failed : this indicates there was some sort of failure that occured
      while the job was running.  It only corresponds to the SLURM states F and
      NF.
    \item  waiting : this indicates that the job is waiting to use resources,
      sleeping or in the queue.  This corresponds to SLURM's PD state, PBS's Q,
      W, S and H states, and the UNIX command {\em ps}'s W, S and D states.
    \item  running : this indicates that the job is actually running.  This
      corresponds to the R state in all implemented batch managers and UNIX
      command {\em ps}.
    \end{itemizer}
  \item  {\em mpi\_job} : this allows us to do special handling for mpi jobs as
    opposed to serial jobs.
  \item  {\em Jpype} : this is the location of Jpype on the system.  Jpype is
    needed to send events to the portal.
  \item  {\em event\_channel} : this is the location of the event channel.  It
    is needed to publish events that can be picked up by the portal.
  \end{itemizer}
\item Member functions: {\em submit\_job}, {\em monitor\_job}, {\em
  remove\_from\_q}/{\em kill\_job}, {\em \_\_init\_\_}.  All the functions can
  print debugging information to standard output, in addition to, publishing events.
  \begin{itemizer}
  \item  {\em \_\_init\_\_} : reads the configuration file ({\bf
    SWIM\_config}), stores the data in {\em my\_vals}, sets the Jpype location, and
    connects to the event channel.
  \item  {\em submit\_job} : submits the job to the queue or runs the job
    (if no {\em batch\_mgr} is specified).  How, where and what is run depends
    on the values in {\em
     my\_vals} that was set up in the {\em \_\_init\_\_} function.  The {\em
     job\_id} is set at this point to allow the script to monitor and cancel the job
    automatically.
  \item  {\em monitor\_job} : this function will run continuously until the job
    finishes.  It queries the batch manager or OS for the status of the job.
    The {\em job\_id} is typically used to query for the status.  An event with
    the state and CPU time used is published at regular intervals.  The
    interval can be sent in via a paramter in the framework, or the default
    will be used.
  \item  {\em remove\_from\_q}/{\em kill\_job} : these functions will remove
    the job from the queue if it has not run yet or kill the job if it is
    currently running.  If the job has finished, the function will not do
    anything.  {\em kill\_job} just calls {\em remove\_from\_q}.
  \end{itemizer}
\end{itemizer}

Table \ref{states} shows the correspondances among the states as defined by
the different tools.  ``PROCS'' simply uses Unix processes rather than a batch
manager, and SWIM is the provided batch negotiator.
  \begin{table}
    \begin{center}
      \begin{tabular}{||l|c|c|c||} \hline \hline
        {\bf SWIM}      &  {\bf SLURM}      & {\bf PBS}  & {\bf PROCS} \\ \hline \hline
		done            &  CD,TO          &  E          &  X,Z            \\  \hline
		trans           &  CG              &  T          &  T            \\ \hline
		failed          &  F,NF            &             &               \\ \hline
		waiting         &  PD              &  Q,W,S,H    &  W,S,D        \\ \hline
		running         &  R               &  R          &  R            \\ \hline \hline
      \end{tabular}
    \end{center}
    \caption{\label{states} Possible Run States for a SWIM Component}
  \end{table}
  
Three Python dictionaries translate native batch manager states
to a uniform SWIM state idenfication:
  \begin{itemizer}
  \item  SLURM\_STATES = \{``CD'':``done'', ``TO'':``done'', ``CG'':``trans'', ``F'':``failed'',
    ``NF'':``failed'', ``PD'':``waiting'', ``R'':``running''\}
  \item PROC\_STATES = \{``X'':``done'', ``Z'':``done'', ``T'':``trans'', ``W'':``waiting'',
    ``S'':``waiting'', ``D'':``waiting'', ``R'':``running''\}
  \item PBS\_STATES = \{``E'':``done'', ``T'':``trans'', ``Q'':``waiting'', ``S'':``waiting'',
    ``W'':``waiting'', ``H'':``waiting'', ``R'':``running''\}
  \end{itemizer}  


