\section{Example Component Setup: Scale in LSA}

The example LSA application has components for BasicInfo, Scale, Reorder,
Splib, and SuperLU. For the Scale component, the contents of the five 
necessary SWIM files is: \\
{\bf SWIM\_component\_required}:  
\begin{verbatim}
Scale   = none
methods = save
mat.in  = notify
\end{verbatim}
{\bf SWIM\_component\_results}:  
\begin{verbatim}
mat.out         = save
summary.html    = save
\end{verbatim}
{\bf SWIM\_component\_files}:  
\begin{verbatim}
SWIM_files                  = none
SWIM_component_required     = none
SWIM_component_results      = none
SWIM_config                 = none
SWIM_runit.py               = none
\end{verbatim}
{\bf SWIM\_component\_config}:  
\begin{verbatim}
Jpype = san/fsp/lib/python
batch_mgr = ssh
executable = ./Scale
num_nodes = 1
\end{verbatim}
Recall that for the file listings, none means take no action and save asks
the framework data manager to archive and track the file.
The component execution file {\bf SWIM\_runit} for Scale
is in Listing \ref{scalerunit}.
\lstset{basicstyle=\small,
    language=Python,
    stringstyle=\ttfamily,
    emph={Scale_services}, emphstyle=\color{red},
    emph={[2]initialize,step,finalize}, emphstyle={[2]\color{blue}}
    }
\begin{lstlisting}[frame=single,
                       caption={SWIM\_runit file for Scale component in LSA},
                       label=scalerunit]

#---------------
# Global modules
#---------------
import os, sys
from time import time, ctime  
#--------------
# Local modules
#--------------
sys.path.append('../PythonUtils')
from FSPutils import publish_event

import Framework

# Allows for debugging print statments when requested
debug_on = False

if len(sys.argv) > 1:
    if sys.argv[1] == "-SWIM_debug":
        debug_on = True

Scale_services = Framework.Services()

#----------------------------------------------------------------
# Set up a subdirectory to run the component in, and put required
# files within that directory.  General initialization.
#----------------------------------------------------------------

newdir = Scale_services.initialize('Scale', debug = debug_on)

#------------------
# Run the component
#------------------

Scale_services.step('Scale', newdir, debug = debug_on)

#------------------------------------
# Clean up and finalize the component
#------------------------------------

Scale_services.finalize('Scale', newdir, debug = debug_on)

\end{lstlisting}
In red are the interactions with the Framework: first a ``framework services'' object
is created for Scale, then it is used to initialize, step (run), and finalize. 
Part of initialization is to create a new subdirectory to run this particular instance
in and then that is used in calls to step and finalize. Most scripts should be
almost this simple. An example of one that does additional processing is Splib,
which generates several graph files. After calling finalize, Splib has the
code fragment in Listing \ref{splibextra} to read a list of graph files, and use {\em matplotlib}
to create PNG files of them.
\lstset{basicstyle=\footnotesize,
    stringstyle=\ttfamily,
    language=Python,
    emph={Scale_services}, emphstyle=\color{red},
    emph={[2]initialize,step,finalize}, emphstyle={[2]\color{blue}},
    showstringspaces=false
    }
\begin{lstlisting}[frame=single,
    caption={Splib extra code for creating graph files},
                       label=splibextra]
#---------------------------------------------------------------------------
# Postprocessing local to this component. This could be done by subclassing
# the services object, but this shows it's not necessary to follow the exact
# same run script for each component. This processes some xgraph files made
# of the convergence of iterative solvers, by converting them for the python
# package matplotlib and then using it to create PNG images of the graphs.
#---------------------------------------------------------------------------
try:
    import matplotlib
    matplotlib.use('Agg')
    try:
        from pylab import *
    
        try:
            os.chdir(newdir)
        except:
            print '*** Cannot chdir to working subdirectory'
    
        try:
            import cvt
        except:
            print '*** cvt not found on import'
    
        try:
            graph_listing = open('graph_files', 'r')
        except: 
            print '*** listing of files to graph not found'
    
        for line in graph_listing.readlines():
            cvt.cvt(line[:-1])
        graph_listing.close()
    except:
        print 'Cannot convert plotfiles to PNG'
        print 'error msg:', sys.exc_info()[0]
except:
    print 'Cannot convert plotfiles to PNG'
    print "error msg returned was ", sys.exc_info()[0]
\end{lstlisting}

\section{Batch Manager Script}

The batch manager negotiator script described in Section \ref{sec:jobmanager}:
\lstset{basicstyle=\scriptsize,
    language=Python,
    stringstyle=\ttfamily,
    emph={Scale_services}, emphstyle=\color{red},
    emph={[2]initialize,step,finalize}, emphstyle={[2]\color{blue}},
    showstringspaces=false
    }
\begin{lstlisting}[frame=single,
    caption={batch\_mgmt\_script.py}, label=batchscript]
"""
---------------------------------
States and their correspondances.
---------------------------------

  SWIM  |  SLURM  |      PBS      |  PROCS
--------------------------------------------
  done  |  CD,TO  |   E           |  X,Z
 trans  |   CG    |   T           |  T
 failed |  F,NF   |               |
waiting |   PD    |   Q, W, S, H  |  W,S,D
running |   R     |   R           |  R

"""

import sys
import os
import time
import subprocess as sp
import Events

SLURM_STATES = {"CD":"done",
 "TO":"done",
 "CG":"trans",
 "F":"failed",
 "NF":"failed",
 "PD":"waiting",
 "R":"running"}

PROC_STATES = {"X":"done",
 "Z":"done",
 "T":"trans",
 "W":"waiting",
 "S":"waiting",
 "D":"waiting",
 "R":"running"}

PBS_STATES = {"E":"done",
 "T":"trans",
 "Q":"waiting",
 "S":"waiting",
 "W":"waiting",
 "H":"waiting",
 "R":"running"}

class fsp_job:   #our new job class
    # A dictionary containing configuration information read from the config file
    my_vals = { "batch_mgr" : "",
                "num_nodes" : 1,
                "executable" : "hostname",
                "jobid" : "0",
                "status" : "done",
                "mpi_job" : False,
                "Jpype" : "/usr/bin",
                "event_channel" : "shortly.cs.indiana.edu:12345",
                "utilpath" : "."
                }

    def parse_config (self):
        try:
            # This assumes that the config file is in the current dir
            config_file = open("SWIM_config", "r")
            
            if config_file:
                line = config_file.readline().strip()
            else:
                line = ""
                
            while line:
                name, val = line.split("=")
                name = name.strip()
                val = val.strip()
                if self.my_vals.has_key(name):
                    if name == "num_nodes":
                        self.my_vals[name] = int(val)
                    elif name == "mpi_job":
                        self.my_vals[name] = bool(val)
                    elif name == "Jpype":
                        self.my_vals[name] = os.path.expanduser(val)
                    else:
                        self.my_vals[name] = val
                else:
                    print "unrecognized name in name-value pair: %s = %s\n" \
                         % (name, val)
                if config_file:
                    line = config_file.readline().strip()
                else:
                    line = ""
                        
            config_file.close()
        except Exception, ex:
            print "problems parsing config file: %s" % ex
                               
                
    #states = something.... see notes above
    def submit_job(self, debug_on = False):
        # submits job to batch_mgr, if one is present
        try:
            if self.my_vals["batch_mgr"] == "SLURM":
                if self.my_vals["mpi_job"]:
                    # "sam_run" calls mpi_run.py if it is an mpi job 
                    cmd = "srun -N %d -b sam_run" % self.my_vals["num_nodes"]
                else:
                    # Grab nodes, run, and grab stderr to get the jobid
                    cmd = "srun -N %d -b %s" % (self.my_vals["num_nodes"], \
                           self.my_vals["executable"])   #assume it has ./ if needed

                #using subprocess
                p1 = sp.Popen(cmd, shell=True, stderr=sp.PIPE)
                s = p1.stderr.read()
                g = s.split()
                self.my_vals["jobid"] = g[2]
                self.my_vals["status"] = g[3]
                
            elif self.my_vals["batch_mgr"] == "PBS":
                scriptfile = open("scriptfile", "w")
                scriptfile.write("#!/bin/tcsh\n")
                scriptfile.write("#PBS -l ncpus=" + str(self.my_vals["num_nodes"]) + "\n")
                scriptfile.write("#PBS -l walltime=00:05:00\n")
                scriptfile.write("#PBS -l mem=50mb\n")
                scriptfile.write("#PBS -r n\n")
                scriptfile.write("#PBS -N FSPjobname\n")
                scriptfile.write("#PBS -q sgi\n")
                scriptfile.write("#PBS -V\n")
                scriptfile.write("cd " + os.getcwd() + "\n")
                scriptfile.write(self.my_vals["executable"] + "\n")
                scriptfile.close()
                #cmd = "qsub -l nodes=" + self.my_vals["num_nodes"] + " scriptfile"
                cmd = "qsub scriptfile"
                os.system("pwd")
                p1 = sp.Popen(cmd, shell=True, stderr=sp.PIPE, stdout=sp.PIPE)
                s = p1.stdout.read()
                g = s.split(".")
                self.my_vals["jobid"] = g[0]

            else:
                #just run
                # need to get pid... storing it in jobid should be fine
                cmd = self.my_vals["executable"] 

                #new version using subprocess
                #print "starting the script"
                p1 = sp.Popen(cmd, shell=True)
                #print "done running script"
                self.my_vals["jobid"] = str(p1.pid)

            if debug_on:
                print cmd
                print "my jobid is: %s" % self.my_vals["jobid"]
            
            message = "job %s started" % self.my_vals["jobid"]           
            Events.publish_event(message, topic='FSP_job')
        except Exception, ex:
            print "submit_job failed with exception %s" % ex
        
    def monitor_job(self, interval=1, debug_on = False):
        # checks the status of the job
        try:
            
            if self.my_vals["batch_mgr"] == "SLURM":
                cmd = "squeue -j " + self.my_vals["jobid"] + " -h -o \" %.2t %.10M\" "

                #new: the squeue command prints to stdout and stderr
                #extract status from output
                p1 = sp.Popen(cmd, shell=True, stdout=sp.PIPE, stderr=sp.PIPE)
                out = p1.stdout.read()
                while out:
                    s, t = out.split()
                    s = s.strip()
                    t = t.strip()
                    self.my_vals["status"] = SLURM_STATES[s]
                    message = "The status of job %s is %s. -- The cpu time is %s."  % \
                        (self.my_vals["jobid"], self.my_vals["status"], t)
                    if debug_on:
                        print "monitor:\n" + message
                    Events.publish_event(message,topic='FSP_job')
                    time.sleep(interval)
                    p1 = sp.Popen(cmd, shell=True, stdout=sp.PIPE, stderr=sp.PIPE)
                    out = p1.stdout.read()
                
            elif self.my_vals["batch_mgr"] == "PBS":
                cmd = "qstat -r %s" % self.my_vals["jobid"]

                #extract status from output
                p1 = sp.Popen(cmd, shell=True, stdout=sp.PIPE, stderr=sp.PIPE)
                out = p1.stdout.read()
                while out:
                    lines = out.split("\n")
                    info = lines[5].split()
                    s, t = info[9], info[10]
                    s = s.strip()
                    t = t.strip()
                    self.my_vals["status"] = PBS_STATES[s]
                    message = "The status of job %s is %s. -- The cpu time is %s."  \
                        % (self.my_vals["jobid"], self.my_vals["status"], t)
                    if debug_on:
                        print "monitor:\n" + message
                    Events.publish_event(message,topic='FSP_job')
                    time.sleep(interval)
                    p1 = sp.Popen(cmd, shell=True, stdout=sp.PIPE, stderr=sp.PIPE)
                    out = p1.stdout.read()

            else:
                cmd = "ps -o s,cputime --pid %s --no-heading" %  self.my_vals["jobid"]
                
                p1 = sp.Popen(cmd, shell=True, stdout=sp.PIPE, stderr=sp.PIPE)
                out = p1.stdout.read()
                while out:
                    s, t = out.split()
                    s = s.strip()
                    t = t.strip()
                    self.my_vals["status"] = PROC_STATES[s[0]]
                    message = "The status of job %s is %s. -- The cpu time is %s."  % \
                        (self.my_vals["jobid"], self.my_vals["status"], t)
                    if debug_on:
                        print "monitor:\n" + message
                    Events.publish_event(message,topic='FSP_job')
                    time.sleep(interval)
                    p1 = sp.Popen(cmd, shell=True, stdout=sp.PIPE, stderr=sp.PIPE)
                    out = p1.stdout.read()

            self.my_vals["status"] = "done"
            message = "The status of job %s is %s." % \
                       (self.my_vals["jobid"], self.my_vals["status"])
            if debug_on:
                print "monitor:\n" + message
            Events.publish_event(message,topic='FSP_job')
        except Exception, ex:
            print "monitor_job failed with exception %s" % ex
                
    def remove_from_q(self, debug_on = False):
        #removes the job from the queue
        # scancel for SLURM, and qdel for PBS
        # scancel and qdel will remove the job from the queue if waiting
        try:
            if self.my_vals["batch_mgr"] == "SLURM":
                cmd = "scancel " + self.my_vals["jobid"]
            elif self.my_vals["batch_mgr"] == "PBS":
                cmd = "qdel " + self.my_vals["jobid"]
            else:
                #kill -9
                cmd = "kill -9 " + self.my_vals["jobid"]
                os.system(cmd)                
                cmd = "ps -f -p " + self.my_vals["jobid"]
                
            p1 = sp.Popen(cmd, shell=True, stdout=sp.PIPE, stderr=sp.PIPE)
            out = p1.stdout.read()
            message = out
            if debug_on:
                print "rfq:\n" + out
            Events.publish_event(message,topic='FSP_job')
        except Exception, ex:
            print "remove_from_q failed with exception %s" % ex
            
    def kill_job(self, debug_on = False):
        #kills a running job -- which is exactly what remove from queue does
        self.remove_from_q(debug_on)
            
    def __init__(self):
        #gets the info from the config file
        self.parse_config()

        #set the path to Jpype 
        sys.path.append(self.my_vals["Jpype"])

        try:
            Events.set_default_broker(self.my_vals["event_channel"])
        except Exception, ex:
            print "events not working: %s" % ex

if __name__ == "__main__":
    my_job = fsp_job()

    my_job.submit_job()
    time.sleep(2)
    my_job.monitor_job(0.5)
    time.sleep(2)
    my_job.kill_job()

\end{lstlisting}

